{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 5: Regresión Logística - Un tutorial sobre Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Para completar el trabajo de este notebook, se recomienda ver el material siguiente:\n",
    "- Videos Youtube: Curso de Andrew Ng sobre la Regresion Logística\n",
    "    - https://www.youtube.com/watch?v=-la3q9d7AKQ (Lecture 6.1 — Logistic Regression | Classification : 8 minutes)\n",
    "    - https://www.youtube.com/watch?v=t1IT5hZfS48 (Lecture 6.2 — Logistic Regression | Hypothesis Representation : 7 minutes)\n",
    "    - https://www.youtube.com/watch?v=F_VG4LNjZZw (Lecture 6.3 — Logistic Regression | Decision Boundary : 14 minutes)\n",
    "    - https://www.youtube.com/watch?v=HIQlmHxI6-0 (Lecture 6.4 — Logistic Regression | Cost Function : 11 minutes)\n",
    "    - https://www.youtube.com/watch?v=TTdcc21Ko9A (Lecture 6.5 — Logistic Regression | Simplified Cost Function And Gradient Descent : 10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "La regresión logística es el algoritmo de clasificación lineal para problemas de dos clases.\n",
    "\n",
    "En este tutorial, descubrirá cómo implementar la regresión logística con el descenso de gradiente estocástico (stochastic gradient descent) desde cero con Python.\n",
    "\n",
    "Después de completar este tutorial, usted sabrá:\n",
    "\n",
    "- Cómo hacer predicciones con un modelo de regresión logística.\n",
    "- Cómo estimar los coeficientes utilizando el descenso de gradiente estocástico.\n",
    "- Cómo aplicar la regresión logística a un problema real de predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "Esta sección dará una breve descripción de la técnica de regresión logística, el descenso por gradiente estocástico y el dataset \"diabetes\" que utilizaremos en este tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "La regresión logística se denomina así por la función utilizada en el núcleo del método, la función logística.\n",
    "\n",
    "La regresión logística utiliza una ecuación como representación, muy similar a la regresión lineal. Los valores de entrada (X) se combinan linealmente utilizando pesos o valores de coeficiente para predecir un valor de salida (y).\n",
    "\n",
    "Una diferencia clave con respecto a la regresión lineal es que el valor de salida que se está modelando es un valor binario (0 ó 1) en lugar de un valor numérico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>yhat = e^(b0 + b1 * x1) / (1 + e^(b0 + b1 * x1))</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto puede simplificarse como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>yhat = 1.0 / (1.0 + e^(-(b0 + b1 * x1)))</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde <b>e</b> es la base de los logaritmos naturales (el número de Euler), <b>yhat</b> es el resultado previsto, <b>b0</b> es el sesgo o término de intercepción y <b>b1</b> es el coeficiente para el valor de entrada simple (<b>x1</b>).\n",
    "\n",
    "La predicción <b>yhat</b> es un valor real entre 0 y 1, que necesita ser redondeado a un valor entero y mapeado a un valor de clase predicho.\n",
    "\n",
    "Cada columna de los datos de entrada tiene un coeficiente <b>b</b> asociado (un valor real constante) que debe aprenderse de los datos de entrenamiento. La representación real del modelo que almacenarías en memoria o en un archivo son los coeficientes de la ecuación.\n",
    "\n",
    "Los coeficientes del algoritmo de regresión logística deben estimarse a partir de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso estocástico por gradiente (Stochastic Gradient Descent)\n",
    "\n",
    "El descenso por gradiente es el proceso de minimizar una función siguiendo los gradientes de la función de coste.\n",
    "\n",
    "Esto implica conocer la forma del coste así como la derivada para que desde un punto dado conozcas la pendiente y puedas moverte en esa dirección, por ejemplo, cuesta abajo hacia el valor mínimo.\n",
    "\n",
    "En el aprendizaje automático, podemos utilizar una técnica que evalúa y actualiza los coeficientes en cada iteración llamada descenso estocástico de gradiente para minimizar el error de un modelo en nuestros datos de entrenamiento.\n",
    "\n",
    "La forma en que funciona este algoritmo de optimización es que cada instancia de entrenamiento se muestra al modelo una por una. El modelo hace una predicción para una instancia de entrenamiento, se calcula el error y se actualiza el modelo para reducir el error para la siguiente predicción.\n",
    "\n",
    "Este procedimiento se puede utilizar para encontrar el conjunto de coeficientes en un modelo que da como resultado el menor error para el modelo en los datos de entrenamiento. En cada iteración, los coeficientes (b) se actualizan utilizando la ecuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>b = b + learning_rate * (y - yhat) * yhat * (1 - yhat) * x</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde <b>b</b> es el coeficiente o peso que se está optimizando, <b>learning_rate</b> es una velocidad de aprendizaje que se debe configurar (por ejemplo, 0.01), <b>(y - yhat)</b> es el error de predicción para el modelo en los datos de entrenamiento atribuidos al peso, <b>yhat</b> es la predicción hecha por los coeficientes y <b>x</b> es el valor de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Problema de clasificación\n",
    "\n",
    "El dataset \"diabetes.csv\" consiste en predecir el inicio de la diabetes en un plazo de 5 años en los indios Pima a los que se les dan detalles médicos básicos.\n",
    "\n",
    "Es un problema de clasificación binaria, donde la predicción es 0 (sin diabetes) o 1 (diabetes).\n",
    "\n",
    "Contiene 768 filas y 9 columnas. Todos los valores en el archivo son numéricos, específicamente valores en coma flotante. Abajo hay una pequeña muestra de las primeras filas del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>6,148,72,35,0,33.6,0.627,50,1\n",
    "1,85,66,29,0,26.6,0.351,31,0\n",
    "8,183,64,0,0,23.3,0.672,32,1\n",
    "1,89,66,23,94,28.1,0.167,21,0\n",
    "0,137,40,35,168,43.1,2.288,33,1\n",
    "...</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "Este tutorial está dividido en 3 partes.\n",
    "\n",
    "    - Hacer predicciones.\n",
    "    - Estimar los Coeficientes de la regresión logística\n",
    "    - Predecir el diabete.\n",
    "\n",
    "Esto le proporcionará la base que necesita para implementar y aplicar la regresión logística con descenso de gradiente estocástico sobre sus propios problemas de modelado predictivo.\n",
    "\n",
    "### 1. Hacer predicciones\n",
    "\n",
    "El primer paso es desarrollar una función que pueda hacer predicciones.\n",
    "\n",
    "Esto será necesario tanto en la evaluación de los valores de los coeficientes candidatos en el descenso del gradiente estocástico como después de que se finalice el modelo y queramos empezar a hacer predicciones sobre los datos de prueba o nuevos datos.\n",
    "\n",
    "Abajo hay una función llamada predict() que predice un valor de salida para una fila con un conjunto de coeficientes.\n",
    "\n",
    "El primer coeficiente en es siempre la intercepción, también llamado el sesgo o b0 ya que es independiente y no es responsable de un valor de entrada específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear un pequeño conjunto de datos para probar nuestra función predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test predictions\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PREGUNTA:</b>\n",
    "\n",
    "con Pandas y Matplotlib, construir un gráfico en dos dimensiones (x1 y x2), posicionando cada punto del dataset y diferenciando los puntos de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar coeficientes (b0, b1, b2) previamente preparados para hacer predicciones para este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef = [-0.406605464, 0.852573316, -1.104746259]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendo todo esto junto podemos probar nuestra función predict() a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0.000, Predicted=0.299 [0]\n",
      "Expected=0.000, Predicted=0.146 [0]\n",
      "Expected=0.000, Predicted=0.085 [0]\n",
      "Expected=0.000, Predicted=0.220 [0]\n",
      "Expected=0.000, Predicted=0.247 [0]\n",
      "Expected=1.000, Predicted=0.955 [1]\n",
      "Expected=1.000, Predicted=0.862 [1]\n",
      "Expected=1.000, Predicted=0.972 [1]\n",
      "Expected=1.000, Predicted=0.999 [1]\n",
      "Expected=1.000, Predicted=0.905 [1]\n"
     ]
    }
   ],
   "source": [
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PREGUNTA:</b> ¿Cuál es la precisión y el recall de nuestro modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, en la realidad no podemos adivinar los buenos coeficientes b. Necesitamos un metodo para aproximar los mejores coeficientes. Presentamos a continuación el método Stochastic Gradient Descent que apunta a este objetivo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimación de los coeficientes\n",
    "\n",
    "Podemos estimar los valores del coeficiente para nuestros datos de entrenamiento utilizando el descenso de gradiente estocástico.\n",
    "\n",
    "El descenso estocástico por gradiente requiere dos parámetros:\n",
    "\n",
    "    - Tasa de Aprendizaje (Learning rate): Se utiliza para limitar el importe que cada coeficiente se corrige cada vez que se actualiza.\n",
    "    - Épocas (Epochs): El número de veces que se repasan los datos de entrenamiento mientras se actualizan los coeficientes.\n",
    "\n",
    "Estos, junto con los datos de la formación, serán los argumentos de la función.\n",
    "\n",
    "Hay 3 bucles que necesitamos realizar en la función:\n",
    "\n",
    "    - Bucle sobre cada época.\n",
    "    - Bucle sobre cada fila de los datos de entrenamiento de una época.\n",
    "    - Bucle sobre cada coeficiente y actualícelo para una fila en una época.\n",
    "\n",
    "Como puedes ver, actualizamos cada coeficiente para cada fila de los datos de entrenamiento, cada época.\n",
    "\n",
    "Los coeficientes se actualizan en función del error que cometió el modelo. El error se calcula como la diferencia entre el valor de salida esperado y la predicción realizada con los coeficientes candidatos.\n",
    "\n",
    "Hay un coeficiente para ponderar cada atributo de entrada, y éstos se actualizan de forma coherente, por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>b1(t+1) = b1(t) + learning_rate * (y(t) - yhat(t)) * yhat(t) * (1 - yhat(t)) * x1(t)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El coeficiente especial al principio de la lista, también llamado intercepción, se actualiza de forma similar, excepto sin una entrada, ya que no está asociado a un valor de entrada específico:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>b0(t+1) = b0(t) + learning_rate * (y(t) - yhat(t)) * yhat(t) * (1 - yhat(t))</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos juntar todo esto. A continuación se muestra una función llamada <b>coefficients_sgd()</b> que calcula los valores de los coeficientes para un conjunto de datos de entrenamiento utilizando el descenso de gradiente estocástico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver, que además, llevamos la cuenta de la suma del error al cuadrado (un valor positivo) de cada época para que podamos imprimir un bonito mensaje en cada bucle exterior.\n",
    "\n",
    "Podemos probar esta función en el mismo pequeño conjunto de datos creado desde arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate coefficients\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos una mayor tasa de aprendizaje de 0,3 y entrenamos el modelo para 100 épocas, o 100 exposiciones de los coeficientes a todo el conjunto de datos de entrenamiento.\n",
    "\n",
    "Al ejecutar el ejemplo se imprime un mensaje en cada época con la suma del error al cuadrado para esa época y el conjunto final de coeficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.300, error=2.217\n",
      ">epoch=1, lrate=0.300, error=1.613\n",
      ">epoch=2, lrate=0.300, error=1.113\n",
      ">epoch=3, lrate=0.300, error=0.827\n",
      ">epoch=4, lrate=0.300, error=0.623\n",
      ">epoch=5, lrate=0.300, error=0.494\n",
      ">epoch=6, lrate=0.300, error=0.412\n",
      ">epoch=7, lrate=0.300, error=0.354\n",
      ">epoch=8, lrate=0.300, error=0.310\n",
      ">epoch=9, lrate=0.300, error=0.276\n",
      ">epoch=10, lrate=0.300, error=0.248\n",
      ">epoch=11, lrate=0.300, error=0.224\n",
      ">epoch=12, lrate=0.300, error=0.205\n",
      ">epoch=13, lrate=0.300, error=0.189\n",
      ">epoch=14, lrate=0.300, error=0.174\n",
      ">epoch=15, lrate=0.300, error=0.162\n",
      ">epoch=16, lrate=0.300, error=0.151\n",
      ">epoch=17, lrate=0.300, error=0.142\n",
      ">epoch=18, lrate=0.300, error=0.134\n",
      ">epoch=19, lrate=0.300, error=0.126\n",
      ">epoch=20, lrate=0.300, error=0.119\n",
      ">epoch=21, lrate=0.300, error=0.113\n",
      ">epoch=22, lrate=0.300, error=0.108\n",
      ">epoch=23, lrate=0.300, error=0.103\n",
      ">epoch=24, lrate=0.300, error=0.098\n",
      ">epoch=25, lrate=0.300, error=0.094\n",
      ">epoch=26, lrate=0.300, error=0.090\n",
      ">epoch=27, lrate=0.300, error=0.087\n",
      ">epoch=28, lrate=0.300, error=0.084\n",
      ">epoch=29, lrate=0.300, error=0.080\n",
      ">epoch=30, lrate=0.300, error=0.078\n",
      ">epoch=31, lrate=0.300, error=0.075\n",
      ">epoch=32, lrate=0.300, error=0.073\n",
      ">epoch=33, lrate=0.300, error=0.070\n",
      ">epoch=34, lrate=0.300, error=0.068\n",
      ">epoch=35, lrate=0.300, error=0.066\n",
      ">epoch=36, lrate=0.300, error=0.064\n",
      ">epoch=37, lrate=0.300, error=0.062\n",
      ">epoch=38, lrate=0.300, error=0.060\n",
      ">epoch=39, lrate=0.300, error=0.059\n",
      ">epoch=40, lrate=0.300, error=0.057\n",
      ">epoch=41, lrate=0.300, error=0.056\n",
      ">epoch=42, lrate=0.300, error=0.054\n",
      ">epoch=43, lrate=0.300, error=0.053\n",
      ">epoch=44, lrate=0.300, error=0.052\n",
      ">epoch=45, lrate=0.300, error=0.051\n",
      ">epoch=46, lrate=0.300, error=0.050\n",
      ">epoch=47, lrate=0.300, error=0.048\n",
      ">epoch=48, lrate=0.300, error=0.047\n",
      ">epoch=49, lrate=0.300, error=0.046\n",
      ">epoch=50, lrate=0.300, error=0.045\n",
      ">epoch=51, lrate=0.300, error=0.044\n",
      ">epoch=52, lrate=0.300, error=0.044\n",
      ">epoch=53, lrate=0.300, error=0.043\n",
      ">epoch=54, lrate=0.300, error=0.042\n",
      ">epoch=55, lrate=0.300, error=0.041\n",
      ">epoch=56, lrate=0.300, error=0.040\n",
      ">epoch=57, lrate=0.300, error=0.040\n",
      ">epoch=58, lrate=0.300, error=0.039\n",
      ">epoch=59, lrate=0.300, error=0.038\n",
      ">epoch=60, lrate=0.300, error=0.038\n",
      ">epoch=61, lrate=0.300, error=0.037\n",
      ">epoch=62, lrate=0.300, error=0.036\n",
      ">epoch=63, lrate=0.300, error=0.036\n",
      ">epoch=64, lrate=0.300, error=0.035\n",
      ">epoch=65, lrate=0.300, error=0.035\n",
      ">epoch=66, lrate=0.300, error=0.034\n",
      ">epoch=67, lrate=0.300, error=0.033\n",
      ">epoch=68, lrate=0.300, error=0.033\n",
      ">epoch=69, lrate=0.300, error=0.032\n",
      ">epoch=70, lrate=0.300, error=0.032\n",
      ">epoch=71, lrate=0.300, error=0.032\n",
      ">epoch=72, lrate=0.300, error=0.031\n",
      ">epoch=73, lrate=0.300, error=0.031\n",
      ">epoch=74, lrate=0.300, error=0.030\n",
      ">epoch=75, lrate=0.300, error=0.030\n",
      ">epoch=76, lrate=0.300, error=0.029\n",
      ">epoch=77, lrate=0.300, error=0.029\n",
      ">epoch=78, lrate=0.300, error=0.029\n",
      ">epoch=79, lrate=0.300, error=0.028\n",
      ">epoch=80, lrate=0.300, error=0.028\n",
      ">epoch=81, lrate=0.300, error=0.027\n",
      ">epoch=82, lrate=0.300, error=0.027\n",
      ">epoch=83, lrate=0.300, error=0.027\n",
      ">epoch=84, lrate=0.300, error=0.026\n",
      ">epoch=85, lrate=0.300, error=0.026\n",
      ">epoch=86, lrate=0.300, error=0.026\n",
      ">epoch=87, lrate=0.300, error=0.026\n",
      ">epoch=88, lrate=0.300, error=0.025\n",
      ">epoch=89, lrate=0.300, error=0.025\n",
      ">epoch=90, lrate=0.300, error=0.025\n",
      ">epoch=91, lrate=0.300, error=0.024\n",
      ">epoch=92, lrate=0.300, error=0.024\n",
      ">epoch=93, lrate=0.300, error=0.024\n",
      ">epoch=94, lrate=0.300, error=0.024\n",
      ">epoch=95, lrate=0.300, error=0.023\n",
      ">epoch=96, lrate=0.300, error=0.023\n",
      ">epoch=97, lrate=0.300, error=0.023\n",
      ">epoch=98, lrate=0.300, error=0.023\n",
      ">epoch=99, lrate=0.300, error=0.022\n",
      "[-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.3\n",
    "n_epoch = 100\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver cómo el error sigue disminuyendo incluso en la época final. Probablemente podríamos entrenar durante mucho más tiempo (más épocas) o aumentar la cantidad que actualizamos los coeficientes en cada época (ritmo de aprendizaje más alto).\n",
    "\n",
    "Experimente y vea lo que se le ocurre.\n",
    "\n",
    "Ahora, apliquemos este algoritmo en un conjunto de datos real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predicción de la diabetes\n",
    "\n",
    "En esta sección, entrenaremos un modelo de regresión logística utilizando el descenso de gradiente estocástico sobre el conjunto de datos de diabetes.\n",
    "\n",
    "El ejemplo asume que una copia CSV del conjunto de datos está en el directorio de trabajo actual con el nombre de archivo nb4-diabetes.csv.\n",
    "\n",
    "El conjunto de datos se carga primero, los valores de las cadenas se convierten a numéricos y cada columna se normaliza a valores en el rango de 0 a 1. Esto se logra con las funciones <code>helper load_csv()</code> y <code>str_column_to_float()</code> para cargar y preparar el dataset y <code>dataset_minmax()</code> y <code>normalize_dataset()</code> para normalizarlo.\n",
    "\n",
    "Usaremos la validación cruzada de k-fold para estimar el rendimiento del modelo aprendido en datos no vistos. Esto significa que construiremos y evaluaremos los modelos k y estimaremos el rendimiento como el rendimiento medio del modelo. La exactitud de la clasificación se utilizará para evaluar cada modelo. Estos comportamientos se proporcionan en las funciones de ayuda <code>cross_validation_split()</code>, <code>accuracy_metric()</code> y <code>evaluate_algorithm()</code>.\n",
    "\n",
    "Usaremos las funciones <code>predict()</code>, <code>coefficients_sgd()</code> creadas anteriormente y una nueva función <code>logistic_regression()</code> para entrenar el modelo.\n",
    "\n",
    "A continuación se muestra el ejemplo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "epoch:8\n",
      "epoch:9\n",
      "epoch:10\n",
      "epoch:11\n",
      "epoch:12\n",
      "epoch:13\n",
      "epoch:14\n",
      "epoch:15\n",
      "epoch:16\n",
      "epoch:17\n",
      "epoch:18\n",
      "epoch:19\n",
      "epoch:20\n",
      "epoch:21\n",
      "epoch:22\n",
      "epoch:23\n",
      "epoch:24\n",
      "epoch:25\n",
      "epoch:26\n",
      "epoch:27\n",
      "epoch:28\n",
      "epoch:29\n",
      "epoch:30\n",
      "epoch:31\n",
      "epoch:32\n",
      "epoch:33\n",
      "epoch:34\n",
      "epoch:35\n",
      "epoch:36\n",
      "epoch:37\n",
      "epoch:38\n",
      "epoch:39\n",
      "epoch:40\n",
      "epoch:41\n",
      "epoch:42\n",
      "epoch:43\n",
      "epoch:44\n",
      "epoch:45\n",
      "epoch:46\n",
      "epoch:47\n",
      "epoch:48\n",
      "epoch:49\n",
      "epoch:50\n",
      "epoch:51\n",
      "epoch:52\n",
      "epoch:53\n",
      "epoch:54\n",
      "epoch:55\n",
      "epoch:56\n",
      "epoch:57\n",
      "epoch:58\n",
      "epoch:59\n",
      "epoch:60\n",
      "epoch:61\n",
      "epoch:62\n",
      "epoch:63\n",
      "epoch:64\n",
      "epoch:65\n",
      "epoch:66\n",
      "epoch:67\n",
      "epoch:68\n",
      "epoch:69\n",
      "epoch:70\n",
      "epoch:71\n",
      "epoch:72\n",
      "epoch:73\n",
      "epoch:74\n",
      "epoch:75\n",
      "epoch:76\n",
      "epoch:77\n",
      "epoch:78\n",
      "epoch:79\n",
      "epoch:80\n",
      "epoch:81\n",
      "epoch:82\n",
      "epoch:83\n",
      "epoch:84\n",
      "epoch:85\n",
      "epoch:86\n",
      "epoch:87\n",
      "epoch:88\n",
      "epoch:89\n",
      "epoch:90\n",
      "epoch:91\n",
      "epoch:92\n",
      "epoch:93\n",
      "epoch:94\n",
      "epoch:95\n",
      "epoch:96\n",
      "epoch:97\n",
      "epoch:98\n",
      "epoch:99\n",
      "epoch:0\n",
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "epoch:8\n",
      "epoch:9\n",
      "epoch:10\n",
      "epoch:11\n",
      "epoch:12\n",
      "epoch:13\n",
      "epoch:14\n",
      "epoch:15\n",
      "epoch:16\n",
      "epoch:17\n",
      "epoch:18\n",
      "epoch:19\n",
      "epoch:20\n",
      "epoch:21\n",
      "epoch:22\n",
      "epoch:23\n",
      "epoch:24\n",
      "epoch:25\n",
      "epoch:26\n",
      "epoch:27\n",
      "epoch:28\n",
      "epoch:29\n",
      "epoch:30\n",
      "epoch:31\n",
      "epoch:32\n",
      "epoch:33\n",
      "epoch:34\n",
      "epoch:35\n",
      "epoch:36\n",
      "epoch:37\n",
      "epoch:38\n",
      "epoch:39\n",
      "epoch:40\n",
      "epoch:41\n",
      "epoch:42\n",
      "epoch:43\n",
      "epoch:44\n",
      "epoch:45\n",
      "epoch:46\n",
      "epoch:47\n",
      "epoch:48\n",
      "epoch:49\n",
      "epoch:50\n",
      "epoch:51\n",
      "epoch:52\n",
      "epoch:53\n",
      "epoch:54\n",
      "epoch:55\n",
      "epoch:56\n",
      "epoch:57\n",
      "epoch:58\n",
      "epoch:59\n",
      "epoch:60\n",
      "epoch:61\n",
      "epoch:62\n",
      "epoch:63\n",
      "epoch:64\n",
      "epoch:65\n",
      "epoch:66\n",
      "epoch:67\n",
      "epoch:68\n",
      "epoch:69\n",
      "epoch:70\n",
      "epoch:71\n",
      "epoch:72\n",
      "epoch:73\n",
      "epoch:74\n",
      "epoch:75\n",
      "epoch:76\n",
      "epoch:77\n",
      "epoch:78\n",
      "epoch:79\n",
      "epoch:80\n",
      "epoch:81\n",
      "epoch:82\n",
      "epoch:83\n",
      "epoch:84\n",
      "epoch:85\n",
      "epoch:86\n",
      "epoch:87\n",
      "epoch:88\n",
      "epoch:89\n",
      "epoch:90\n",
      "epoch:91\n",
      "epoch:92\n",
      "epoch:93\n",
      "epoch:94\n",
      "epoch:95\n",
      "epoch:96\n",
      "epoch:97\n",
      "epoch:98\n",
      "epoch:99\n",
      "epoch:0\n",
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "epoch:8\n",
      "epoch:9\n",
      "epoch:10\n",
      "epoch:11\n",
      "epoch:12\n",
      "epoch:13\n",
      "epoch:14\n",
      "epoch:15\n",
      "epoch:16\n",
      "epoch:17\n",
      "epoch:18\n",
      "epoch:19\n",
      "epoch:20\n",
      "epoch:21\n",
      "epoch:22\n",
      "epoch:23\n",
      "epoch:24\n",
      "epoch:25\n",
      "epoch:26\n",
      "epoch:27\n",
      "epoch:28\n",
      "epoch:29\n",
      "epoch:30\n",
      "epoch:31\n",
      "epoch:32\n",
      "epoch:33\n",
      "epoch:34\n",
      "epoch:35\n",
      "epoch:36\n",
      "epoch:37\n",
      "epoch:38\n",
      "epoch:39\n",
      "epoch:40\n",
      "epoch:41\n",
      "epoch:42\n",
      "epoch:43\n",
      "epoch:44\n",
      "epoch:45\n",
      "epoch:46\n",
      "epoch:47\n",
      "epoch:48\n",
      "epoch:49\n",
      "epoch:50\n",
      "epoch:51\n",
      "epoch:52\n",
      "epoch:53\n",
      "epoch:54\n",
      "epoch:55\n",
      "epoch:56\n",
      "epoch:57\n",
      "epoch:58\n",
      "epoch:59\n",
      "epoch:60\n",
      "epoch:61\n",
      "epoch:62\n",
      "epoch:63\n",
      "epoch:64\n",
      "epoch:65\n",
      "epoch:66\n",
      "epoch:67\n",
      "epoch:68\n",
      "epoch:69\n",
      "epoch:70\n",
      "epoch:71\n",
      "epoch:72\n",
      "epoch:73\n",
      "epoch:74\n",
      "epoch:75\n",
      "epoch:76\n",
      "epoch:77\n",
      "epoch:78\n",
      "epoch:79\n",
      "epoch:80\n",
      "epoch:81\n",
      "epoch:82\n",
      "epoch:83\n",
      "epoch:84\n",
      "epoch:85\n",
      "epoch:86\n",
      "epoch:87\n",
      "epoch:88\n",
      "epoch:89\n",
      "epoch:90\n",
      "epoch:91\n",
      "epoch:92\n",
      "epoch:93\n",
      "epoch:94\n",
      "epoch:95\n",
      "epoch:96\n",
      "epoch:97\n",
      "epoch:98\n",
      "epoch:99\n",
      "epoch:0\n",
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "epoch:8\n",
      "epoch:9\n",
      "epoch:10\n",
      "epoch:11\n",
      "epoch:12\n",
      "epoch:13\n",
      "epoch:14\n",
      "epoch:15\n",
      "epoch:16\n",
      "epoch:17\n",
      "epoch:18\n",
      "epoch:19\n",
      "epoch:20\n",
      "epoch:21\n",
      "epoch:22\n",
      "epoch:23\n",
      "epoch:24\n",
      "epoch:25\n",
      "epoch:26\n",
      "epoch:27\n",
      "epoch:28\n",
      "epoch:29\n",
      "epoch:30\n",
      "epoch:31\n",
      "epoch:32\n",
      "epoch:33\n",
      "epoch:34\n",
      "epoch:35\n",
      "epoch:36\n",
      "epoch:37\n",
      "epoch:38\n",
      "epoch:39\n",
      "epoch:40\n",
      "epoch:41\n",
      "epoch:42\n",
      "epoch:43\n",
      "epoch:44\n",
      "epoch:45\n",
      "epoch:46\n",
      "epoch:47\n",
      "epoch:48\n",
      "epoch:49\n",
      "epoch:50\n",
      "epoch:51\n",
      "epoch:52\n",
      "epoch:53\n",
      "epoch:54\n",
      "epoch:55\n",
      "epoch:56\n",
      "epoch:57\n",
      "epoch:58\n",
      "epoch:59\n",
      "epoch:60\n",
      "epoch:61\n",
      "epoch:62\n",
      "epoch:63\n",
      "epoch:64\n",
      "epoch:65\n",
      "epoch:66\n",
      "epoch:67\n",
      "epoch:68\n",
      "epoch:69\n",
      "epoch:70\n",
      "epoch:71\n",
      "epoch:72\n",
      "epoch:73\n",
      "epoch:74\n",
      "epoch:75\n",
      "epoch:76\n",
      "epoch:77\n",
      "epoch:78\n",
      "epoch:79\n",
      "epoch:80\n",
      "epoch:81\n",
      "epoch:82\n",
      "epoch:83\n",
      "epoch:84\n",
      "epoch:85\n",
      "epoch:86\n",
      "epoch:87\n",
      "epoch:88\n",
      "epoch:89\n",
      "epoch:90\n",
      "epoch:91\n",
      "epoch:92\n",
      "epoch:93\n",
      "epoch:94\n",
      "epoch:95\n",
      "epoch:96\n",
      "epoch:97\n",
      "epoch:98\n",
      "epoch:99\n",
      "epoch:0\n",
      "epoch:1\n",
      "epoch:2\n",
      "epoch:3\n",
      "epoch:4\n",
      "epoch:5\n",
      "epoch:6\n",
      "epoch:7\n",
      "epoch:8\n",
      "epoch:9\n",
      "epoch:10\n",
      "epoch:11\n",
      "epoch:12\n",
      "epoch:13\n",
      "epoch:14\n",
      "epoch:15\n",
      "epoch:16\n",
      "epoch:17\n",
      "epoch:18\n",
      "epoch:19\n",
      "epoch:20\n",
      "epoch:21\n",
      "epoch:22\n",
      "epoch:23\n",
      "epoch:24\n",
      "epoch:25\n",
      "epoch:26\n",
      "epoch:27\n",
      "epoch:28\n",
      "epoch:29\n",
      "epoch:30\n",
      "epoch:31\n",
      "epoch:32\n",
      "epoch:33\n",
      "epoch:34\n",
      "epoch:35\n",
      "epoch:36\n",
      "epoch:37\n",
      "epoch:38\n",
      "epoch:39\n",
      "epoch:40\n",
      "epoch:41\n",
      "epoch:42\n",
      "epoch:43\n",
      "epoch:44\n",
      "epoch:45\n",
      "epoch:46\n",
      "epoch:47\n",
      "epoch:48\n",
      "epoch:49\n",
      "epoch:50\n",
      "epoch:51\n",
      "epoch:52\n",
      "epoch:53\n",
      "epoch:54\n",
      "epoch:55\n",
      "epoch:56\n",
      "epoch:57\n",
      "epoch:58\n",
      "epoch:59\n",
      "epoch:60\n",
      "epoch:61\n",
      "epoch:62\n",
      "epoch:63\n",
      "epoch:64\n",
      "epoch:65\n",
      "epoch:66\n",
      "epoch:67\n",
      "epoch:68\n",
      "epoch:69\n",
      "epoch:70\n",
      "epoch:71\n",
      "epoch:72\n",
      "epoch:73\n",
      "epoch:74\n",
      "epoch:75\n",
      "epoch:76\n",
      "epoch:77\n",
      "epoch:78\n",
      "epoch:79\n",
      "epoch:80\n",
      "epoch:81\n",
      "epoch:82\n",
      "epoch:83\n",
      "epoch:84\n",
      "epoch:85\n",
      "epoch:86\n",
      "epoch:87\n",
      "epoch:88\n",
      "epoch:89\n",
      "epoch:90\n",
      "epoch:91\n",
      "epoch:92\n",
      "epoch:93\n",
      "epoch:94\n",
      "epoch:95\n",
      "epoch:96\n",
      "epoch:97\n",
      "epoch:98\n",
      "epoch:99\n",
      "Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229]\n",
      "Mean Accuracy: 77.124%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    " \n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        print(\"epoch:\"+str(epoch))\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            #print(\"coef_0:\"+str(coef[0]))\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef\n",
    " \n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)\n",
    " \n",
    "# Test the logistic regression algorithm on the diabetes dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'nb5-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizó un valor k de 5 para la validación cruzada, dando a cada muestra 768/5 = 153,6 o algo más de 150 registros para ser evaluados en cada iteración. Se eligió una tasa de aprendizaje de 0,1 y 100 épocas de entrenamiento con un poco de experimentación.\n",
    "\n",
    "Puedes probar tus propias configuraciones y ver si puedes superar mi puntuación.\n",
    "\n",
    "Al ejecutar este ejemplo, se imprimen las puntuaciones de cada uno de los 5 pliegues de validación cruzada y, a continuación, se imprime la precisión de clasificación media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pregunta</b>:\n",
    "\n",
    "Con Pandas y Matplotlib, dibujar una curva que representa la tasa de error de la regresión logística según el epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
